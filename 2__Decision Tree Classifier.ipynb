{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Machine Learning -- Writing a Decision Tree Classifier</h2>  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "This notebook is completed entirely in Python.<br>\n",
    "    Farnam Adelkhani -- Last update: April 2nd, 2019\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Statistics Review for Decision Trees</h2>  \n",
    "\n",
    "<center><b>Decision Trees:</b> Start at root node, move to internal nodes, lastly end at leaf nodes.</center>\n",
    "\n",
    "### Two Main Types of Decision Trees Exist:\n",
    "> **Classification Trees** :  \n",
    "When the predicted outcome is the class to which the data belongs.  \n",
    "**Regression Trees** :  \n",
    "When the predicted outcome can be considered a real number.  \n",
    "(ie: Price of a car, or wait time at a restaurant).  \n",
    "    \n",
    "<center><b>(CART)</b> - Umbrella term for both Classification And Regression Trees. </center>\n",
    "\n",
    "**Ensemble Learner Methods:**\n",
    "> A machine learning technique that combines several base models in order to produce one optimal predictive model.  \n",
    "(ie: Take a collection of 'ok' predictors and combine them into something more powerful.  \n",
    "\n",
    "**Boosting:** Idea is to combine several weak algorithms into a strong one. Try predictors sequentially, such that each subsequent model attempts to fix the errors of its predecessor.  \n",
    "> 1. Gradient Boosted Trees:  \n",
    "    - Contrary to AdaBoost, which tweaks the instance weights at every interaction, this method tries to fit the new predictor to the residual errors made by the previous predictor.  \n",
    "2. AdaBoost (Adaptive Boost):\n",
    "    - Creating a forest of stumps (ie: 1 Node and 2 leaves)\n",
    "3. Bootstrap Aggregated (Bagging): \n",
    "    - Combines bootstrapping and aggregation to form one ensemble model.\n",
    "4. Rotation Forest:\n",
    "    - Rotation Forest is an ensemble classifier using many decision tree models and it can be used for classification or Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier:\n",
    "- Add a root node for the tree\n",
    "- All nodes receive a list of rows as input.\n",
    "- The root will receive the entire training set.\n",
    "- Each node asks a true/false question about one of the features.\n",
    "- In response split the data into 2 subsets/child nodes.  \n",
    "\n",
    "The key to building a successful tree, is to know what questions to ask and when.  \n",
    "- ie: Quantify how much a Question helps to unmix the labels.  \n",
    "... Can do this with the metric: Gini Impurity\n",
    "\n",
    "Quantify how much a Question reduces an uncertainty.  \n",
    "- Can do this with the metric: Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Coefficient:\n",
    "<h4 align=\"center\">A measure of statistical dispersion intended to represent the income or wealth distribution of a nation's residents,<br> and is the most commonly used measurement of inequality.</h4> \n",
    "<img src=\"Decision_Tree_Classifier--Images/lorenz.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Python 2 / 3 compatability\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dataset.\n",
    "# Format: each row is an example.\n",
    "# The last column is the label.\n",
    "# The first three columns are features.\n",
    "#\n",
    "# Note: The 4th and 10th examples have the same features, but different labels...\n",
    "#   such that we can see how the tree handles this case.\n",
    "#\n",
    "# [\"feature\", \"feature\", \"feature\", \"label\"]\n",
    "\n",
    "training_data = [\n",
    "    ['V8', 'Green', 2018, 'Audi'],\n",
    "    ['EV', 'Black', 2015, 'Benz'],\n",
    "    ['I4', 'White', 2018, 'BMW'],\n",
    "    ['EV', 'Yellow', 2013, 'Ford'], # Same features\n",
    "    ['EV', 'Blue', 2015, 'Honda'],\n",
    "    ['V6', 'White', 2013, 'Honda'],\n",
    "    ['V8', 'Green', 2016, 'Lexus'],\n",
    "    ['V6', 'Red', 2015, 'Lexus'],\n",
    "    ['V8', 'White', 2018, 'Lexus'],\n",
    "    ['EV', 'Yellow', 2013, 'Tesla'], # Same features\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column labels:\n",
    "header = [\"engine\", \"color\", \"year\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unique values for a column in a dataset (unsorted):\n",
    "def unique_vals(rows, col):\n",
    "    return set([row[col] for row in rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EV', 'I4', 'V8', 'V6'}\n",
      "{'Black', 'White', 'Red', 'Green', 'Yellow', 'Blue'}\n",
      "{2016, 2018, 2013, 2015}\n",
      "{'Ford', 'Tesla', 'Lexus', 'Honda', 'Benz', 'Audi', 'BMW'}\n"
     ]
    }
   ],
   "source": [
    "# Demo of unique_vals function for all column:\n",
    "engine = unique_vals(training_data, 0)\n",
    "color = unique_vals(training_data, 1)\n",
    "year = unique_vals(training_data, 2)\n",
    "make = unique_vals(training_data, 3)\n",
    "\n",
    "print(engine, color, year, make, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts(rows):\n",
    "    \"\"\"Count # of each type of label in the dataset.\"\"\"\n",
    "    # Dictionary of labels\n",
    "    counts = {}\n",
    "    for row in rows:\n",
    "        # The label is usually the last column\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Audi': 1, 'Benz': 1, 'BMW': 1, 'Ford': 1, 'Honda': 2, 'Lexus': 3, 'Tesla': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo of class_counts function:\n",
    "class_counts(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(value):\n",
    "    \"\"\"Test if value is numeric.\"\"\"\n",
    "    return isinstance(value, int) or isinstance(value, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Demo of is_numeric function:\n",
    "digit = is_numeric(7) # Expected: True\n",
    "color = is_numeric(\"Red\") # Expected: False\n",
    "\n",
    "print(digit, color, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\"Question class is used to partition a dataset.\n",
    "\n",
    "    This class just records a 'column number'\n",
    "    - (ex: 0 for Engine) and a 'column value' (ex: V6 or EV). \n",
    "    The 'match' method is used to compare the feature value in an example \n",
    "    to the feature value stored in the question.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column, value):\n",
    "        # Represent a question by storing a column # and value \n",
    "        # ... for threshhold used to partition the data\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "\n",
    "    def match(self, example):\n",
    "        # Compare the feature value in an example to the\n",
    "        # ... feature value in this question.\n",
    "        val = example[self.column]\n",
    "        # Check if val is numerical:\n",
    "        if is_numeric(val):\n",
    "            return val >= self.value\n",
    "        else:\n",
    "            return val == self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        # This is just a helper method to print\n",
    "        # ... the question in a readable format.\n",
    "        condition = \"==\"\n",
    "        if is_numeric(self.value):\n",
    "            condition = \">=\"\n",
    "        return \"Is %s %s %s?\" % (\n",
    "            header[self.column], condition, str(self.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is year >= 2015?"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo of Question class:\n",
    "# Let's write a question for a numeric attribute, is year >= 2015?\n",
    "Question(2, 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is color == Green?"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo of Question class:\n",
    "# Ask if the color is green\n",
    "q = Question(1, 'Green')\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's pick an example from the training set...\n",
    "# 0 refers to the 1st row, not column\n",
    "example = training_data[0]\n",
    "# print(example)\n",
    "# ... and see if it matches the question\n",
    "q.match(example) # this will be true, since the first row contains \"Green\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(rows, question):\n",
    "    \"\"\"Partitions a dataset by row.\n",
    "\n",
    "    For each row in the dataset, check if it matches the question. \n",
    "    If so:\n",
    "        add it to 'true rows'\n",
    "    else: \n",
    "        add it to 'false rows'.\n",
    "    \"\"\"\n",
    "    true_rows, false_rows = [], []\n",
    "    for row in rows:\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['V8', 'Green', 2018, 'Audi'], ['V8', 'Green', 2016, 'Lexus']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo of partition function:\n",
    "# Partition the training data based on whether rows contain Green.\n",
    "true_rows, false_rows = partition(training_data, Question(1, 'Green'))\n",
    "\n",
    "# This will contain all the 'Green' rows.\n",
    "true_rows\n",
    "# This will contain everything else.\n",
    "# false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(rows):\n",
    "    \"\"\"Calculate the Gini Impurity for a list of rows.\n",
    "       ie: How much uncertainty there is at a node.\n",
    "\n",
    "    There are different means of acheiving this, this is one way:\n",
    "    Wikipedia Link:\n",
    "    https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n",
    "    \"\"\"\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl**2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Demo to understand how Gini Impurity works:\n",
    "\n",
    "# First, a dataset with no mixing.\n",
    "no_mixing = [['Ford'],\n",
    "             ['Ford']] #This should return 0\n",
    "\n",
    "# Now, a dataset with a 50:50 Honda:Lexus ratio\n",
    "mixing = [['Honda'],\n",
    "          ['Lexus']] #This should return 0.5\n",
    "\n",
    "# No mixing\n",
    "print(gini(no_mixing))\n",
    "\n",
    "# Mixing\n",
    "print(gini(mixing)) \n",
    "   # 'mixing' will return 0.5, meaning, there's a 50% chance of misclassifying a random example we draw from the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Decision_Tree_Classifier--Images/gini_impurity.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "<h3 align=\"center\">ie: We have a 1/5 chance of being correct if randomly assign an object to a label.</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999999999999998"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the dataset pictured above:\n",
    "lots_of_mixing = [['Apple'],\n",
    "                  ['Banana'],\n",
    "                  ['Grape'],\n",
    "                  ['Lemon'],\n",
    "                  ['Orange'],]\n",
    "\n",
    "# This will return 0.8\n",
    "gini(lots_of_mixing)\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(left, right, current_uncertainty):\n",
    "    \"\"\"Information Gain: Find the question that reduces uncertainty the most.\n",
    "\n",
    "    uncertainty of the starting node - the weighted impurity of two child nodes.\n",
    "    \"\"\"\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_uncertainty - p * gini(left) - (1 - p) * gini(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo:\n",
    "# Calculate the uncertainy of training data.\n",
    "current_uncertainty = gini(training_data)\n",
    "current_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11999999999999988"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much information do we gain by partioning on 'Yellow'?\n",
    "true_rows, false_rows = partition(training_data, Question(1, 'Yellow'))\n",
    "info_gain(true_rows, false_rows, current_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04857142857142871"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What about if we partioned on 'White' instead?\n",
    "true_rows, false_rows = partition(training_data, Question(1,'White'))\n",
    "info_gain(true_rows, false_rows, current_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EV', 'Yellow', 2013, 'Ford'], ['EV', 'Yellow', 2013, 'Tesla']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It looks like we learned more using 'Yellow' (0.119), than 'White' (0.048).\n",
    "# Why? Look at the different splits that result, and see which one\n",
    "# looks more 'unmixed' to you.\n",
    "true_rows, false_rows = partition(training_data, Question(1,'Yellow'))\n",
    "\n",
    "# Here, the true_rows contain only 'Tesla'.\n",
    "true_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['V8', 'Green', 2018, 'Audi'],\n",
       " ['EV', 'Black', 2015, 'Benz'],\n",
       " ['I4', 'White', 2018, 'BMW'],\n",
       " ['EV', 'Blue', 2015, 'Honda'],\n",
       " ['V6', 'White', 2013, 'Honda'],\n",
       " ['V8', 'Green', 2016, 'Lexus'],\n",
       " ['V6', 'Red', 2015, 'Lexus'],\n",
       " ['V8', 'White', 2018, 'Lexus']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And the false rows contain multiple vehicle manufacturers:\n",
    "false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EV', 'Blue', 2015, 'Honda']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On the other hand, partitioning by Blue doesn't help so much.\n",
    "true_rows, false_rows = partition(training_data, Question(1,'Blue'))\n",
    "\n",
    "# We've isolated one 'Honda' in the true rows.\n",
    "true_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['V8', 'Green', 2018, 'Audi'],\n",
       " ['EV', 'Black', 2015, 'Benz'],\n",
       " ['I4', 'White', 2018, 'BMW'],\n",
       " ['EV', 'Yellow', 2013, 'Ford'],\n",
       " ['V6', 'White', 2013, 'Honda'],\n",
       " ['V8', 'Green', 2016, 'Lexus'],\n",
       " ['V6', 'Red', 2015, 'Lexus'],\n",
       " ['V8', 'White', 2018, 'Lexus'],\n",
       " ['EV', 'Yellow', 2013, 'Tesla']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But, the false-rows are badly mixed up.\n",
    "false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(rows):\n",
    "    \"\"\"Find the best question to ask by iterating over every feature / value\n",
    "    and calculating the information gain.\"\"\"\n",
    "    best_gain = 0  # keep track of the best information gain\n",
    "    best_question = None  # keep train of the feature / value that produced it\n",
    "    current_uncertainty = gini(rows)\n",
    "    n_features = len(rows[0]) - 1  # number of columns\n",
    "\n",
    "    # Iterate over every value for the features:\n",
    "    for col in range(n_features):\n",
    "\n",
    "        # unique values in the column\n",
    "        values = set([row[col] for row in rows])\n",
    "\n",
    "        # for each value\n",
    "        for val in values:\n",
    "\n",
    "            # Generate a question for that feature:\n",
    "            question = Question(col, val)\n",
    "\n",
    "            # Try splitting the dataset, ie: partition the data on it:\n",
    "            true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "            # Discard any questions that fail to produce a split:\n",
    "            # Take a weighted average of the impurity of each set:\n",
    "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate the information gain from this split\n",
    "            gain = info_gain(true_rows, false_rows, current_uncertainty)\n",
    "\n",
    "            # You actually can use '>' instead of '>=' here\n",
    "            # but I wanted the tree to look a certain way for our\n",
    "            # toy dataset.\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_question = gain, question\n",
    "\n",
    "    return best_gain, best_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is engine == EV?"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "# Find the best question to ask first for our toy dataset.\n",
    "best_gain, best_question = find_best_split(training_data)\n",
    "best_question\n",
    "# FYI: is color == Red is just as good. See the note in the code above\n",
    "# where I used '>='.\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    \"\"\"A Leaf node classifies data.\n",
    "\n",
    "    This holds a dictionary of class (e.g., \"Apple\") -> number of times\n",
    "    it appears in the rows from the training data that reach this leaf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = class_counts(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Node:\n",
    "    \"\"\"A Decision Node asks a question.\n",
    "\n",
    "    This holds a reference to the question, and to the two child nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 true_branch,\n",
    "                 false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(rows):\n",
    "    \"\"\"Builds the tree.\n",
    "\n",
    "    Rules of recursion: 1) Believe that it works. 2) Start by checking\n",
    "    for the base case (no further information gain). 3) Prepare for\n",
    "    giant stack traces.\n",
    "    \"\"\"\n",
    "\n",
    "    # Try partitioing the dataset on each of the unique attribute,\n",
    "    # calculate the information gain,\n",
    "    # and return the question that produces the highest gain.\n",
    "    # Find the best question to ask at this node:\n",
    "    gain, question = find_best_split(rows)\n",
    "\n",
    "    # Base case: no further info gain\n",
    "    # Since we can ask no further questions,\n",
    "    # we'll return a leaf.\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "\n",
    "    # If we reach here, we have found a useful feature / value\n",
    "    # to partition on.\n",
    "    true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "    # Recursively build the true branch.\n",
    "    true_branch = build_tree(true_rows)\n",
    "\n",
    "    # Recursively build the false branch.\n",
    "    false_branch = build_tree(false_rows)\n",
    "\n",
    "    # Return a Question node.\n",
    "    # This records the best feature / value to ask at this point,\n",
    "    # as well as the branches to follow\n",
    "    # dependingo on the answer.\n",
    "    return Decision_Node(question, true_branch, false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, spacing=\"\"):\n",
    "    \"\"\"World's most elegant tree printing function.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        print (spacing + \"Predict\", node.predictions)\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    print (spacing + str(node.question))\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    print (spacing + '--> True:')\n",
    "    print_tree(node.true_branch, spacing + \"  \")\n",
    "\n",
    "    # Call this function recursively on the false branch\n",
    "    print (spacing + '--> False:')\n",
    "    print_tree(node.false_branch, spacing + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When calling 'build_tree' it receives entire training tree as input\n",
    "# my_tree = Returns ref to root node of tree:\n",
    "my_tree = build_tree(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is engine == EV?\n",
      "--> True:\n",
      "  Is color == Black?\n",
      "  --> True:\n",
      "    Predict {'Benz': 1}\n",
      "  --> False:\n",
      "    Is color == Yellow?\n",
      "    --> True:\n",
      "      Predict {'Ford': 1, 'Tesla': 1}\n",
      "    --> False:\n",
      "      Predict {'Honda': 1}\n",
      "--> False:\n",
      "  Is year >= 2015?\n",
      "  --> True:\n",
      "    Is engine == I4?\n",
      "    --> True:\n",
      "      Predict {'BMW': 1}\n",
      "    --> False:\n",
      "      Is year >= 2018?\n",
      "      --> True:\n",
      "        Is color == Green?\n",
      "        --> True:\n",
      "          Predict {'Audi': 1}\n",
      "        --> False:\n",
      "          Predict {'Lexus': 1}\n",
      "      --> False:\n",
      "        Predict {'Lexus': 2}\n",
      "  --> False:\n",
      "    Predict {'Honda': 1}\n"
     ]
    }
   ],
   "source": [
    "print_tree(my_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To Do: Visualizing the decision tree above:\n",
    "# Load libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from IPython.display import Image  \n",
    "from sklearn import tree\n",
    "import pydotplus\n",
    "\n",
    "# Create decision tree classifer object\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify data, starts with reference to root node:\n",
    "\n",
    "def classify(row, node):\n",
    "    \"\"\"See the 'rules of recursion' above.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.predictions\n",
    "\n",
    "    # Decide whether to follow the true-branch or the false-branch.\n",
    "    # Compare the feature / value stored in the node,\n",
    "    # to the example we're considering.\n",
    "    if node.question.match(row):\n",
    "        return classify(row, node.true_branch)\n",
    "    else:\n",
    "        return classify(row, node.false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Audi': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo:\n",
    "# The tree predicts the 1st row of our\n",
    "# training data is an Audi with confidence 1.\n",
    "classify(training_data[0], my_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_leaf(counts):\n",
    "    \"\"\"A nicer way to print the predictions at a leaf.\"\"\"\n",
    "    total = sum(counts.values()) * 1.0\n",
    "    probs = {}\n",
    "    for lbl in counts.keys():\n",
    "        probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Audi': '100%'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print utilizing a percentage:\n",
    "print_leaf(classify(training_data[0], my_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ford': '50%', 'Tesla': '50%'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo:\n",
    "# On the 4th row, the confidence is lower\n",
    "# This is because the same features are shared with another label\n",
    "print_leaf(classify(training_data[3], my_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "testing_data = [\n",
    "    ['V8', 'Green', 2018, 'Audi'],\n",
    "    ['EV', 'Black', 2015, 'Benz'],\n",
    "    ['I4', 'White', 2018, 'BMW'],\n",
    "    ['EV', 'Yellow', 2013, 'Ford'], # Same features\n",
    "    ['EV', 'Blue', 2015, 'Honda'],\n",
    "    ['V6', 'White', 2013, 'Honda'],\n",
    "    ['V8', 'Green', 2016, 'Lexus'],\n",
    "    ['V6', 'Red', 2015, 'Lexus'],\n",
    "    ['V8', 'White', 2018, 'Lexus'],\n",
    "    ['EV', 'Yellow', 2013, 'Tesla'], # Same features\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Audi. Predicted: {'Audi': '100%'}\n",
      "Actual: Benz. Predicted: {'Benz': '100%'}\n",
      "Actual: BMW. Predicted: {'BMW': '100%'}\n",
      "Actual: Ford. Predicted: {'Ford': '50%', 'Tesla': '50%'}\n",
      "Actual: Honda. Predicted: {'Honda': '100%'}\n",
      "Actual: Honda. Predicted: {'Honda': '100%'}\n",
      "Actual: Lexus. Predicted: {'Lexus': '100%'}\n",
      "Actual: Lexus. Predicted: {'Lexus': '100%'}\n",
      "Actual: Lexus. Predicted: {'Lexus': '100%'}\n",
      "Actual: Tesla. Predicted: {'Ford': '50%', 'Tesla': '50%'}\n"
     ]
    }
   ],
   "source": [
    "for row in testing_data:\n",
    "    print (\"Actual: %s. Predicted: %s\" %\n",
    "           (row[-1], print_leaf(classify(row, my_tree))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Credits:\n",
    "Josh Gordon @ Google: https://github.com/random-forests/tutorials\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
